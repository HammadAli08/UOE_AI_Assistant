# UOE AI Assistant â€“ Comprehensive Technical Report & Architecture Documentation

## 1. Project Overview
The **UOE AI Assistant** is a production-grade Retrieval-Augmented Generation (RAG) system built for the University of Education (UOE), Lahore. It serves as an intelligent academic assistant, strictly answering queries using official university documents (schemes of studies, course outlines, regulations, and fee structures). 

The application is structured into a High-Performance FastAPI Backend and a visually stunning, responsive React/Vite Frontend.

---

## 2. System Architecture

### High-Level Architecture Diagram
```mermaid
graph TD
    User([User / Browser]) <-->|HTTP / React| Frontend[Frontend React SPA]
    Frontend <-->|REST API + Server-Sent Events| Backend[FastAPI Backend]
    
    subgraph Backend Infrastructure
        Backend -->|Query| Router[Request Router]
        Router --> Pipeline[RAG Pipeline Orchestrator]
        Pipeline --> Enhancer[Query Enhancer GPT-4o-mini]
        Pipeline --> Memory[(Redis Short-Term Memory)]
        Pipeline --> Retriever[Vector Retriever]
        Retriever <-->|MCP Client Search| Pinecone[(Pinecone Vector DB)]
        Pipeline --> SmartRAG[Smart RAG Module]
        Pipeline --> Generator[Response Generator GPT-4o / Claude]
    end
    
    subgraph Data Layer
        Documents[Raw PDFs / Word Docs] --> Ingestion[Data Ingestion Script]
        Ingestion -->|Chunking & Embedding| Pinecone
    end
```

---

## 3. Frontend Architecture (React + Vite + Tailwind)

The frontend is a single-page application focused on providing a premium "Awwwards-level" user experience with a cinematic dark theme, glassmorphism, and complex micro-animations.

### Core Technologies
*   **Framework:** React 18, Vite
*   **Styling:** Vanilla CSS & Tailwind CSS (used for layout utility). Custom keyframe animations for glows, typing waves, and scrollbars.
*   **State Management:** Zustand (`useChatStore.js`) for global chat state + React Context where applicable.
*   **Routing:** React Router DOM (Landing Page `/` vs Chat Interface `/chat`).

### Frontend Component Flow
```mermaid
graph TD
    App[App.jsx] --> HeroPage[Hero/Landing Page]
    App --> ChatPage[Chat Application]
    
    ChatPage --> Header[Header / Navbar]
    ChatPage --> ChatContainer[Chat Container]
    ChatPage --> ChatInput[Chat Input Area]
    
    ChatContainer --> WelcomeScreen[Welcome Screen / Empty State]
    ChatContainer --> MsgBubble[Message Bubble]
    ChatContainer --> StreamBubble[Streaming Message Bubble]
    ChatContainer --> TypingInd[Typing Indicator Wave]
    
    MsgBubble --> SourcesPanel[Dynamic Source Citations Panel]
```

### Key Frontend Features
1.  **Dynamic Chat Layout:** The input stays centered on the welcome screen and dynamically docks to the bottom once the first message is sent.
2.  **Streaming Content:** Server-Sent Events (SSE) parse chunked tokens from the backend for real-time text appearance, paired with a custom CSS "breathing cursor."
3.  **Source Attribution:** Document sources are beautifully attached below AI responses via `SourcesPanel`, mapping metadata to visual cards with color-coded relevance scores.

---

## 4. Backend Architecture (FastAPI + LangChain)

The backend handles the heavy lifting of RAG. It strictly enforces namespace isolation to ensure a BS student isn't given MS rules.

### Core Technologies
*   **Framework:** FastAPI
*   **Core RAG:** Custom pipeline utilizing LangChain primitives.
*   **Vector Database:** Pinecone (accessed via MCP protocol/client).
*   **Memory:** Redis Cloud for session-based conversational history handling.
*   **Tracing:** LangSmith for deep operational observability.

### NameSpace Isolation Mapping
The knowledge base is strictly partitioned into namespaces:
*   `bs-adp`: BS & ADP schemes, fees, course outlines.
*   `ms-phd`: MS & PhD research, admissions, faculty.
*   `rules`: General university regulations, exams, grading.

### Standard RAG Workflow (Stream & Query)
```mermaid
sequenceDiagram
    participant F as Frontend
    participant P as Pipeline
    participant M as Redis Memory
    participant E as Query Enhancer
    participant R as Retriever
    participant DB as Pinecone DB
    participant G as Generator

    F->>P: POST /chat {query, namespace}
    P->>M: Fetch chat_history (last 2 turns)
    P->>E: enhance(query, chat_history)
    E-->>P: Returns clean, resolved query
    P->>R: retrieve(enhanced_query, namespace)
    R->>DB: Dense + Sparse Search
    DB-->>R: Top-K Chunks
    R-->>P: Cleaned Documents + Metadata
    P->>G: generate(query, docs, namespace, history)
    G-->>F: Stream tokens (SSE)
```

---

## 5. The "Smart RAG" Pipeline Module

When `enable_smart=True`, the system shifts from a linear fetch-and-generate approach to an aggressive, self-correcting agent loop.

### Smart Workflow Description
1.  **Enhance & Retrieve:** The user's query is enhanced and initial documents are fetched.
2.  **Grader (LLM-as-a-judge):** Every chunk is strictly graded (0 or 1) against the user's *original* query to prevent hallucination.
3.  **Threshold Check:** 
    *   If enough relevant chunks are found -> Proceed to Generator.
    *   If insufficient -> Pass irrelevant chunks to `Rewriter`.
4.  **Rewrite & Retry Loop:** The query is aggressively rewritten to search for different semantic angles. This loops up to **6 times**. The system accumulates good chunks across every pass.
5.  **Best-Effort Fallback:** If the loop exhausts and some chunks exist (but less than the ideal threshold), it answers anyway utilizing what it found. If exactly *zero* chunks are found, it checks if user clarification is needed before issuing a soft "Sorry".

### Smart RAG Diagram
```mermaid
stateDiagram-v2
    [*] --> EnhanceQuery
    EnhanceQuery --> Retrieve
    Retrieve --> GradeChunks
    
    GradeChunks --> CheckThreshold
    CheckThreshold --> GenerateAnswer : Enough Good Chunks
    
    CheckThreshold --> RewriteQuery : Insufficient Chunks
    RewriteQuery --> Retrieve : Retry (Max 6)
    
    CheckThreshold --> ClarifyUser : Max Retries Hit & 0 Chunks
    CheckThreshold --> GenerateAnswer : Max Retries Hit & Some Chunks (Best Effort)
    
    GenerateAnswer --> [*]
    ClarifyUser --> [*]
```

---

## 6. System Prompts & Guardrails

The system relies heavily on carefully constructed prompts located in `backend/system_prompts/`. 

*   **Query Enhancer Prompt:** Takes the user's conversational input and transforms it into a keyword-dense database search query. Crucially, it uses chat history to resolve pronouns (e.g., "What is its outline?" -> "Syllabus outline Compiler Construction COMP3149").
*   **Domain Prompts (`bs_adp`, `ms_phd`, `rules`):** These enforce strict grounding. A core rule introduced is the **Exact Match Directive** for structured content. If a user asks for an outline, objective, or weekly topic breakdown, the LLM is instructed to bypass summarization entirely and reproduce the text *verbatim* from the chunk.

---

## 7. Data Ingestion Pipeline

### Data Flow
1. **Raw PDFs/Docs:** Official curriculum documents are loaded.
2. **Text Splitting / Chunking:** Documents are broken down into semantically meaningful chunks with overlapping windows to preserve context boundaries.
3. **Metadata Annotation:** Every chunk is tagged with its source file, page number, program, and department.
4. **Embedding:** The chunks are passed through the embedding model to convert text to high-dimensional vectors.
5. **Upsert:** Vectors + Metadata are upserted into the specific Pinecone namespace corresponding to their domain.

---

## 8. Deployment Considerations

*   **Environment Variables:** Security relies on `.env` injecting `OPENAI_API_KEY`, `PINECONE_API_KEY`, and `REDIS_` credentials.
*   **Latency Optimizations:** The Query Enhancer and Grader tasks intentionally use `gpt-4o-mini` instead of standard `gpt-4o` to drastically reduce system latency without sacrificing operational logic.
*   **API Exposure:** Routes are protected under `/api/v1/`, ready to be configured with JWT-based Auth when rolling out.

*Report generated automatically for project reference.*
